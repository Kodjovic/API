@app.delete("/cache")
def clear_cache():
    """Vide le cache"""
    CACHE["data"] = []
    CACHE["last_update"] = None
    CACHE["is_updating"] = False
    return {"message": "Cache vidÃ© avec succÃ¨s"}

# DonnÃ©es de fallback en cas d'Ã©chec total du scraping
FALLBACK_DATA = [
    {
        "Nom_pharmacie": "Pharmacie de l'Ã‰toile",
        "Numero_telephone": "22 21 27 64",
        "Adresse": "Rue de l'IndÃ©pendance, LomÃ©"
    },
    {
        "Nom_pharmacie": "Pharmacie du Centre",
        "Numero_telephone": "22 21 35 42",
        "Adresse": "Avenue du 24 Janvier, LomÃ©"
    },
    {
        "Nom_pharmacie": "Pharmacie de la Paix",
        "Numero_telephone": "22 21 45 67",
        "Adresse": "Boulevard du 13 Janvier, LomÃ©"
    }
]

@app.get("/fallback_data")
def get_fallback_data():
    """Retourne des donnÃ©es de fallback pour tests"""
    return {
        "data": FALLBACK_DATA,
        "metadata": {
            "source": "fallback",
            "count": len(FALLBACK_DATA),
            "note": "DonnÃ©es de test - remplacer par scraping rÃ©el"
        }
    }from fastapi import FastAPI, BackgroundTasks
from bs4 import BeautifulSoup
import requests
import re
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
from contextlib import asynccontextmanager

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cache global pour stocker les donnÃ©es
CACHE = {
    "data": [],
    "last_update": None,
    "cache_duration_minutes": 30,  # AugmentÃ© Ã  30 minutes
    "is_updating": False  # Flag pour Ã©viter les requÃªtes multiples
}

# Configuration des requÃªtes
REQUEST_CONFIG = {
    'timeout': aiohttp.ClientTimeout(total=30, connect=10, sock_read=20),
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
    }
}

def nettoyer_texte(texte: str) -> str:
    """Nettoie et normalise le texte"""
    if not texte:
        return ""
    
    texte = texte.strip()
    texte = texte.replace("â˜Ž", "").replace("Ã¢ËœÅ½", "").replace("Ã‚", "")
    texte = re.sub(r"\s{2,}", " ", texte)
    texte = re.sub(r"[^\w\s\-\+\(\)\.\/,]", "", texte)
    return texte

def is_cache_valid() -> bool:
    """VÃ©rifie si le cache est encore valide"""
    if not CACHE["last_update"] or not CACHE["data"]:
        return False
    
    elapsed = datetime.now() - CACHE["last_update"]
    return elapsed.total_seconds() < (CACHE["cache_duration_minutes"] * 60)

async def scraper_pharmacies_async() -> List[Dict]:
    """Scraping asynchrone des pharmacies avec retry et multiples stratÃ©gies"""
    urls_to_try = [
        "https://www.inam.tg/pharmacies-de-garde/",
        "http://www.inam.tg/pharmacies-de-garde/",  # Fallback HTTP
    ]
    
    # DiffÃ©rents User-Agents Ã  essayer
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
    ]
    
    for attempt in range(3):  # 3 tentatives
        for url in urls_to_try:
            for user_agent in user_agents:
                try:
                    logger.info(f"Tentative {attempt + 1}: {url} avec UA: {user_agent[:50]}...")
                    
                    # Configuration spÃ©cifique pour cette tentative
                    timeout = aiohttp.ClientTimeout(
                        total=45,  # Timeout total augmentÃ©
                        connect=15,  # Timeout de connexion
                        sock_read=30  # Timeout de lecture
                    )
                    
                    headers = REQUEST_CONFIG['headers'].copy()
                    headers['User-Agent'] = user_agent
                    
                    # Connector avec SSL dÃ©sactivÃ© si nÃ©cessaire
                    connector = aiohttp.TCPConnector(
                        limit=30,
                        ttl_dns_cache=300,
                        use_dns_cache=True,
                        verify_ssl=False  # DÃ©sactiver SSL pour les sites avec certificats problÃ©matiques
                    )
                    
                    async with aiohttp.ClientSession(
                        timeout=timeout,
                        connector=connector,
                        headers=headers
                    ) as session:
                        
                        # Essayer avec delay progressif
                        if attempt > 0:
                            await asyncio.sleep(2 ** attempt)  # 2, 4, 8 secondes
                        
                        async with session.get(url, allow_redirects=True) as response:
                            logger.info(f"Status: {response.status}, Headers: {dict(response.headers)}")
                            
                            if response.status == 200:
                                html_content = await response.text()
                                logger.info(f"Contenu HTML rÃ©cupÃ©rÃ© ({len(html_content)} caractÃ¨res)")
                                
                                # VÃ©rifier si on a du contenu utile
                                if len(html_content) > 1000:  # Minimum de contenu attendu
                                    pharmacies = parse_html_content(html_content)
                                    if pharmacies:
                                        logger.info(f"âœ… SuccÃ¨s avec {url}: {len(pharmacies)} pharmacies")
                                        return pharmacies
                                    else:
                                        logger.warning(f"HTML rÃ©cupÃ©rÃ© mais aucune pharmacie trouvÃ©e")
                                        # Log d'un Ã©chantillon du HTML pour debug
                                        logger.debug(f"Ã‰chantillon HTML: {html_content[:500]}...")
                                else:
                                    logger.warning(f"Contenu HTML trop court: {len(html_content)} caractÃ¨res")
                            
                            elif response.status in [403, 406]:
                                logger.warning(f"AccÃ¨s refusÃ© ({response.status}), tentative avec autre User-Agent")
                                continue
                            else:
                                logger.warning(f"HTTP {response.status}: {response.reason}")
                
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout avec {url} (tentative {attempt + 1})")
                    continue
                except aiohttp.ClientError as e:
                    logger.warning(f"Erreur client avec {url}: {e}")
                    continue
                except Exception as e:
                    logger.warning(f"Erreur avec {url}: {e}")
                    continue
    
    logger.error("âŒ Toutes les tentatives de scraping ont Ã©chouÃ©")
    return []

def parse_html_content(html_content: str) -> List[Dict]:
    """Parse le contenu HTML et extrait les pharmacies"""
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        
        # Log de la structure pour debug
        logger.info(f"Titre de la page: {soup.title.string if soup.title else 'N/A'}")
        
        # Multiples stratÃ©gies de recherche
        table = None
        strategies = [
            lambda: soup.find('table'),
            lambda: soup.find('div', class_='pharmacies'),
            lambda: soup.find('div', id='pharmacies'),
            lambda: soup.find('div', class_='table-responsive'),
            lambda: soup.find('tbody'),
            lambda: soup.select('.pharmacy-list tr'),
            lambda: soup.select('table tr'),
        ]
        
        for i, strategy in enumerate(strategies):
            try:
                result = strategy()
                if result:
                    table = result
                    logger.info(f"âœ… StratÃ©gie {i+1} rÃ©ussie: {table.name}")
                    break
            except Exception as e:
                logger.debug(f"StratÃ©gie {i+1} Ã©chouÃ©e: {e}")
                continue
        
        if not table:
            logger.error("âŒ Aucune table/structure trouvÃ©e avec toutes les stratÃ©gies")
            # Log des balises principales pour debug
            main_tags = [tag.name for tag in soup.find_all()[:20]]
            logger.debug(f"Principales balises trouvÃ©es: {main_tags}")
            return []
        
        pharmacies = []
        
        # Traitement selon le type d'Ã©lÃ©ment trouvÃ©
        if table.name == 'table' or table.name == 'tbody':
            rows = table.find_all('tr')[1:] if table.find_all('tr') else []
        else:
            rows = table.find_all('div', class_='pharmacy-row') if hasattr(table, 'find_all') else []
        
        logger.info(f"TrouvÃ© {len(rows)} lignes Ã  traiter")
        
        for i, row in enumerate(rows):
            try:
                if row.find_all('td'):
                    cells = row.find_all('td')
                    if len(cells) >= 3:
                        nom = nettoyer_texte(cells[0].get_text())
                        tel = nettoyer_texte(cells[1].get_text())
                        adresse = nettoyer_texte(cells[2].get_text())
                    else:
                        continue
                else:
                    # StratÃ©gie alternative
                    nom = nettoyer_texte(row.find(class_='nom').get_text() if row.find(class_='nom') else "")
                    tel = nettoyer_texte(row.find(class_='tel').get_text() if row.find(class_='tel') else "")
                    adresse = nettoyer_texte(row.find(class_='adresse').get_text() if row.find(class_='adresse') else "")
                
                if nom and len(nom) > 2:  # Validation plus stricte
                    pharmacies.append({
                        "Nom_pharmacie": nom,
                        "Numero_telephone": tel,
                        "Adresse": adresse
                    })
                    
            except Exception as e:
                logger.warning(f"Erreur ligne {i}: {e}")
                continue
        
        logger.info(f"âœ… Parsing terminÃ©: {len(pharmacies)} pharmacies extraites")
        return pharmacies
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors du parsing HTML: {e}")
        return []

async def update_cache_background():
    """Met Ã  jour le cache en arriÃ¨re-plan"""
    if CACHE["is_updating"]:
        logger.info("Mise Ã  jour dÃ©jÃ  en cours, ignorÃ©e")
        return
    
    CACHE["is_updating"] = True
    try:
        pharmacies = await scraper_pharmacies_async()
        if pharmacies:
            CACHE["data"] = pharmacies
            CACHE["last_update"] = datetime.now()
            logger.info(f"Cache mis Ã  jour avec {len(pharmacies)} pharmacies")
        else:
            logger.warning("Aucune donnÃ©e rÃ©cupÃ©rÃ©e lors de la mise Ã  jour")
    except Exception as e:
        logger.error(f"Erreur lors de la mise Ã  jour du cache: {e}")
    finally:
        CACHE["is_updating"] = False

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("ðŸš€ DÃ©marrage de l'API - PrÃ©-chargement des donnÃ©es...")
    await update_cache_background()
    
    # DÃ©marrage d'une tÃ¢che de mise Ã  jour pÃ©riodique
    async def periodic_update():
        while True:
            await asyncio.sleep(CACHE["cache_duration_minutes"] * 60)
            if not is_cache_valid():
                await update_cache_background()
    
    task = asyncio.create_task(periodic_update())
    
    yield
    
    # Shutdown
    task.cancel()
    try:
        await task
    except asyncio.CancelledError:
        pass

app = FastAPI(lifespan=lifespan)

@app.get("/")
def home():
    return {
        "message": "API de pharmacies de garde en ligne ðŸš‘",
        "status": "active",
        "endpoints": {
            "/pharmacies_de_garde": "RÃ©cupÃ©rer les pharmacies de garde",
            "/pharmacies_de_garde?force_update=true": "Forcer la mise Ã  jour",
            "/status": "Statut du cache",
            "/health": "VÃ©rification de santÃ©"
        }
    }

@app.get("/health")
def health_check():
    """Endpoint de vÃ©rification de santÃ© pour Render"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "cache_status": is_cache_valid(),
        "pharmacies_count": len(CACHE["data"])
    }

@app.get("/pharmacies_de_garde")
async def pharmacies_de_garde(background_tasks: BackgroundTasks, force_update: bool = False):
    """
    RÃ©cupÃ¨re les pharmacies de garde avec cache intelligent
    """
    
    # Si le cache est valide et qu'on ne force pas la mise Ã  jour
    if is_cache_valid() and not force_update:
        return {
            "data": CACHE["data"],
            "metadata": {
                "source": "cache",
                "last_update": CACHE["last_update"].isoformat(),
                "count": len(CACHE["data"]),
                "cache_expires_in_minutes": round(CACHE["cache_duration_minutes"] - 
                    (datetime.now() - CACHE["last_update"]).total_seconds() / 60, 2)
            }
        }
    
    # Si on force la mise Ã  jour ou si le cache n'est pas valide
    if force_update or not is_cache_valid():
        # Lancer la mise Ã  jour en arriÃ¨re-plan pour les prochaines requÃªtes
        background_tasks.add_task(update_cache_background)
        
        # Si on a des donnÃ©es en cache (mÃªme expirÃ©es), les retourner immÃ©diatement
        if CACHE["data"]:
            return {
                "data": CACHE["data"],
                "metadata": {
                    "source": "cache_stale",
                    "last_update": CACHE["last_update"].isoformat() if CACHE["last_update"] else None,
                    "count": len(CACHE["data"]),
                    "note": "DonnÃ©es en cache retournÃ©es, mise Ã  jour en cours en arriÃ¨re-plan"
                }
            }
    
    # Si vraiment aucune donnÃ©e n'est disponible, faire une requÃªte synchrone
    try:
        pharmacies = await scraper_pharmacies_async()
        if pharmacies:
            CACHE["data"] = pharmacies
            CACHE["last_update"] = datetime.now()
            
            return {
                "data": pharmacies,
                "metadata": {
                    "source": "fresh_data",
                    "last_update": CACHE["last_update"].isoformat(),
                    "count": len(pharmacies),
                    "cache_duration_minutes": CACHE["cache_duration_minutes"]
                }
            }
        else:
            return {
                "data": [],
                "metadata": {
                    "source": "error",
                    "error": "Aucune donnÃ©e trouvÃ©e sur le site source",
                    "count": 0,
                    "suggestion": "Le site source peut Ãªtre temporairement indisponible"
                }
            }
            
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration des donnÃ©es: {e}")
        return {
            "data": [],
            "metadata": {
                "source": "error",
                "error": f"Erreur technique: {str(e)}",
                "count": 0,
                "suggestion": "RÃ©essayez dans quelques minutes"
            }
        }

@app.get("/status")
def get_status():
    """Retourne le statut dÃ©taillÃ© du cache et des donnÃ©es"""
    next_update = 0
    if CACHE["last_update"]:
        elapsed_minutes = (datetime.now() - CACHE["last_update"]).total_seconds() / 60
        next_update = max(0, CACHE["cache_duration_minutes"] - elapsed_minutes)
    
    return {
        "cache_status": {
            "is_cache_valid": is_cache_valid(),
            "is_updating": CACHE["is_updating"],
            "last_update": CACHE["last_update"].isoformat() if CACHE["last_update"] else None,
            "cached_pharmacies_count": len(CACHE["data"]),
            "cache_duration_minutes": CACHE["cache_duration_minutes"],
            "next_update_in_minutes": round(next_update, 2)
        },
        "system_info": {
            "timestamp": datetime.now().isoformat(),
            "version": "2.0.0"
        }
    }

@app.post("/force_update")
async def force_update():
    """Force la mise Ã  jour immÃ©diate du cache"""
    await update_cache_background()
    return {
        "message": "Mise Ã  jour forcÃ©e terminÃ©e",
        "count": len(CACHE["data"]),
        "last_update": CACHE["last_update"].isoformat() if CACHE["last_update"] else None
    }

@app.get("/test_scraping")
async def test_scraping():
    """Endpoint de test pour diagnostiquer les problÃ¨mes de scraping"""
    url = "https://www.inam.tg/pharmacies-de-garde/"
    
    results = {
        "url": url,
        "tests": [],
        "final_result": None
    }
    
    # Test de connectivitÃ© basique
    try:
        import socket
        socket.setdefaulttimeout(10)
        host = "www.inam.tg"
        port = 443
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex((host, port))
        sock.close()
        
        connectivity_test = {
            "test": "connectivity",
            "success": result == 0,
            "details": f"Connexion TCP vers {host}:{port} {'rÃ©ussie' if result == 0 else 'Ã©chouÃ©e'}"
        }
        results["tests"].append(connectivity_test)
        
    except Exception as e:
        results["tests"].append({
            "test": "connectivity",
            "success": False,
            "error": str(e)
        })
    
    # Test HTTP basique
    try:
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(verify_ssl=False)
        ) as session:
            async with session.get(url) as response:
                http_test = {
                    "test": "http_basic",
                    "success": response.status == 200,
                    "status": response.status,
                    "headers": dict(response.headers),
                    "content_length": len(await response.text()) if response.status == 200 else 0
                }
                results["tests"].append(http_test)
                
    except Exception as e:
        results["tests"].append({
            "test": "http_basic",
            "success": False,
            "error": str(e)
        })
    
    # Test de scraping complet
    try:
        pharmacies = await scraper_pharmacies_async()
        scraping_test = {
            "test": "full_scraping",
            "success": len(pharmacies) > 0,
            "pharmacies_found": len(pharmacies),
            "sample_data": pharmacies[:2] if pharmacies else None
        }
        results["tests"].append(scraping_test)
        results["final_result"] = pharmacies
        
    except Exception as e:
        results["tests"].append({
            "test": "full_scraping",
            "success": False,
            "error": str(e)
        })
    
    return results
