from fastapi import FastAPI
from bs4 import BeautifulSoup
import requests
import re
from datetime import datetime, timedelta
from typing import List, Dict

app = FastAPI()

# Cache global pour stocker les donn√©es
CACHE = {
    "data": [],
    "last_update": None,
    "cache_duration_minutes": 5  # Cache pendant 5 minutes
}

# Session r√©utilisable pour optimiser les requ√™tes HTTP
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
})

# Nettoyage du texte (inchang√©)
def nettoyer_texte(texte):
    texte = texte.strip()
    texte = texte.replace("‚òé", "").replace("√¢Àú≈Ω", "").replace("√Ç", "")
    return re.sub(r"\s{2,}", " ", texte)

# V√©rifier si le cache est encore valide
def is_cache_valid() -> bool:
    if not CACHE["last_update"]:
        return False
    
    elapsed = datetime.now() - CACHE["last_update"]
    return elapsed.total_seconds() < (CACHE["cache_duration_minutes"] * 60)

# Scraping optimis√© avec gestion d'erreurs
def scraper_pharmacies():
    url = "https://www.inam.tg/pharmacies-de-garde/"
    
    try:
        # Utiliser la session avec timeout
        response = session.get(url, timeout=10)
        response.raise_for_status()  # L√®ve une exception si statut HTTP d'erreur
        
        soup = BeautifulSoup(response.text, "html.parser")
        table = soup.find('table')
        
        if not table:
            print("Aucune table trouv√©e sur la page")
            return []
        
        pharmacies = []
        rows = table.find_all('tr')[1:]  # Skip header
        
        for row in rows:
            cells = row.find_all('td')
            if len(cells) >= 3:
                nom = nettoyer_texte(cells[0].text)
                tel = nettoyer_texte(cells[1].text)
                adresse = nettoyer_texte(cells[2].text)
                
                if adresse:  # V√©rification que l'adresse existe
                    pharmacies.append({
                        "Nom_pharmacie": nom,
                        "Numero_telephone": tel,
                        "Adresse": adresse
                    })
        
        print(f"Scraping r√©ussi: {len(pharmacies)} pharmacies trouv√©es")
        return pharmacies
        
    except requests.exceptions.RequestException as e:
        print(f"Erreur de requ√™te HTTP: {e}")
        return []
    except Exception as e:
        print(f"Erreur de scraping: {e}")
        return []

# Route d'accueil
@app.get("/")
def home():
    return {
        "message": "Bienvenue sur l'API de pharmacies de garde en ligne üöë",
        "endpoints": {
            "/pharmacies_de_garde": "R√©cup√©rer les pharmacies de garde",
            "/pharmacies_de_garde?force_update=true": "Forcer la mise √† jour des donn√©es",
            "/status": "Statut du cache et derni√®re mise √† jour"
        }
    }

# Route principale optimis√©e avec cache
@app.get("/pharmacies_de_garde")
def pharmacies_de_garde(force_update: bool = False):
    """
    R√©cup√®re les pharmacies de garde avec cache intelligent
    
    Args:
        force_update (bool): Si True, force la mise √† jour m√™me si le cache est valide
    
    Returns:
        dict: Donn√©es des pharmacies avec m√©tadonn√©es
    """
    
    # Si le cache est valide et qu'on ne force pas la mise √† jour
    if is_cache_valid() and not force_update and CACHE["data"]:
        return {
            "data": CACHE["data"],
            "metadata": {
                "source": "cache",
                "last_update": CACHE["last_update"].isoformat(),
                "count": len(CACHE["data"]),
                "cache_expires_in_minutes": CACHE["cache_duration_minutes"] - 
                    (datetime.now() - CACHE["last_update"]).total_seconds() / 60
            }
        }
    
    # Sinon, r√©cup√©rer des donn√©es fra√Æches
    try:
        print("R√©cup√©ration de nouvelles donn√©es...")
        pharmacies = scraper_pharmacies()
        
        if pharmacies:
            # Mettre √† jour le cache
            CACHE["data"] = pharmacies
            CACHE["last_update"] = datetime.now()
            
            return {
                "data": pharmacies,
                "metadata": {
                    "source": "fresh_data",
                    "last_update": CACHE["last_update"].isoformat(),
                    "count": len(pharmacies),
                    "cache_duration_minutes": CACHE["cache_duration_minutes"]
                }
            }
        else:
            # Si le scraping √©choue mais qu'on a des donn√©es en cache
            if CACHE["data"]:
                return {
                    "data": CACHE["data"],
                    "metadata": {
                        "source": "cache_fallback",
                        "last_update": CACHE["last_update"].isoformat() if CACHE["last_update"] else None,
                        "count": len(CACHE["data"]),
                        "warning": "Scraping √©chou√©, donn√©es du cache retourn√©es"
                    }
                }
            else:
                return {
                    "data": [],
                    "metadata": {
                        "source": "error",
                        "error": "Impossible de r√©cup√©rer les donn√©es et aucun cache disponible",
                        "count": 0
                    }
                }
                
    except Exception as e:
        # En cas d'erreur, retourner le cache s'il existe
        if CACHE["data"]:
            return {
                "data": CACHE["data"],
                "metadata": {
                    "source": "cache_error_fallback",
                    "last_update": CACHE["last_update"].isoformat() if CACHE["last_update"] else None,
                    "count": len(CACHE["data"]),
                    "error": f"Erreur: {str(e)}"
                }
            }
        else:
            return {
                "data": [],
                "metadata": {
                    "source": "error",
                    "error": str(e),
                    "count": 0
                }
            }

# Route pour obtenir le statut du cache
@app.get("/status")
def get_status():
    """Retourne le statut du cache et des donn√©es"""
    return {
        "cache_status": {
            "is_cache_valid": is_cache_valid(),
            "last_update": CACHE["last_update"].isoformat() if CACHE["last_update"] else None,
            "cached_pharmacies_count": len(CACHE["data"]),
            "cache_duration_minutes": CACHE["cache_duration_minutes"]
        },
        "next_update_in_minutes": max(0, CACHE["cache_duration_minutes"] - 
            (datetime.now() - CACHE["last_update"]).total_seconds() / 60) if CACHE["last_update"] else 0
    }

# Route pour vider le cache (utile pour les tests)
@app.delete("/cache")
def clear_cache():
    """Vide le cache pour forcer la prochaine mise √† jour"""
    CACHE["data"] = []
    CACHE["last_update"] = None
    return {"message": "Cache vid√© avec succ√®s"}

# Pr√©-chargement des donn√©es au d√©marrage de l'API
@app.on_event("startup")
async def startup_event():
    """Pr√©-charge les donn√©es au d√©marrage pour une premi√®re requ√™te rapide"""
    print("üöÄ D√©marrage de l'API - Pr√©-chargement des donn√©es...")
    try:
        pharmacies = scraper_pharmacies()
        if pharmacies:
            CACHE["data"] = pharmacies
            CACHE["last_update"] = datetime.now()
            print(f"‚úÖ Cache initialis√© avec {len(pharmacies)} pharmacies")
        else:
            print("‚ö†Ô∏è  Aucune donn√©e r√©cup√©r√©e au d√©marrage")
    except Exception as e:
        print(f"‚ùå Erreur lors du pr√©-chargement: {e}")
